{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params:\n",
    "    def __init__(self, t, dist, n=10, gen_size = 100, noise=0):\n",
    "        self.t = t\n",
    "        self.dist = dist\n",
    "        self.n = n \n",
    "        self.gen_size = gen_size\n",
    "        self.noise = noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluations are representet as two sets, neg and pos where each varible *representet as an int) is in either neg or pos\n",
    "class Assignment:\n",
    "    def __init__(self, varibles):\n",
    "        self.varibles = varibles\n",
    "        \n",
    "           \n",
    "\n",
    "a1 = [1,1,0,0,0,1,1,0,1,0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hypothesies are representet as two sets neg and pos, where eatch varible can only bee in one set, but can be in neither\n",
    "class Conjunction:\n",
    "    def __init__(self, varibles):\n",
    "        self.varibles = varibles\n",
    "        \n",
    "    \n",
    "    def evaluate(self, evl):\n",
    "        if len(self.varibles) != len(evl):\n",
    "               return False\n",
    "        \n",
    "        mistakes = list(filter(lambda xy: xy[0]!=2 and xy[0]!=xy[1], zip(self.varibles, evl)))\n",
    "        return len(mistakes) < 1\n",
    "    \n",
    "t = Conjunction([1,2,0,2,2,0,2,2,1,2]) \n",
    "t.evaluate(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1618, 1432, 1595, 1761, 1305, 1652, 1355, 1229, 1667, 1517]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 1, 1]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class make_dist():\n",
    "    def __init__(self, mean, std, n=10):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.n = n\n",
    "    \n",
    "    #generates nums that represents Assignmentuations\n",
    "    def normal_dist(self):\n",
    "        #cap so that the binary representations does not contain more digits than n\n",
    "        cap = 2**(self.n+1)-1\n",
    "        minimum  = 2**self.n \n",
    "        normal_num = np.random.normal(self.mean, self.std)\n",
    "        floored_abs = abs(np.math.floor(normal_num))\n",
    "        num_in_range = max(minimum, min(cap, floored_abs))\n",
    "        return num_in_range\n",
    "    \n",
    "    # converts num to useble structure\n",
    "    def num_to_Assignment(self, num):\n",
    "        #comvert to bin\n",
    "        bin_list = [int(x) for x in bin(num)[3:]]\n",
    "        \n",
    "        return bin_list\n",
    "    \n",
    "    \n",
    "    def generate_nums(self, k=1):\n",
    "        return [self.normal_dist() for _ in range(k)]\n",
    "    \n",
    "    def generate_examples(self, k=1):\n",
    "        nums = self.generate_nums(k)\n",
    "        return [self.num_to_Assignment(num) for num in nums]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "dist = make_dist(mean=2**10 *1.5, std=200, n=10)\n",
    "\n",
    "\n",
    "print(dist.generate_nums(10))\n",
    "S = dist.generate_examples(100)\n",
    "S[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = params(t=t, dist=dist, n=10, gen_size=100, noise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return ist of tuples (example, evaluation)\n",
    "def lable_examples(S, t):\n",
    "    return [(e, t.evaluate(e)) for e in S]\n",
    "\n",
    "l_S = lable_examples(S, params1.t)\n",
    "l_S[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 1, 0, 1, 0, 1, 1, 1], False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_S[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 0, 0, 0, 1, 0, 1, 0, 1, 1], True),\n",
       " ([1, 0, 0, 0, 1, 0, 1, 0, 1, 1], True),\n",
       " ([1, 0, 0, 1, 1, 0, 0, 1, 1, 1], True)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x[1],l_S))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flips some of the evaluations\n",
    "def add_noise(labeled_S, noise):\n",
    "    for e in labeled_S:\n",
    "        r = np.random.rand()\n",
    "        if r < noise:\n",
    "            #flips evaluation in tuple\n",
    "            e = (e[0], not e[1])\n",
    "    return labeled_S\n",
    "\n",
    "n_S = add_noise(l_S, params1.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes labeled examples with noise (exept if noise=0).\n",
    "def prepere_noisy_examples(params):\n",
    "    k = params.gen_size\n",
    "    dist = params.dist\n",
    "    S = dist.generate_examples(k)\n",
    "    labeled_S = lable_examples(S, params.t)\n",
    "    noicy_S = add_noise(labeled_S, params.n)\n",
    "    return noicy_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate data from labels\n",
    "def seperate_labels(S):\n",
    "    # mapes X to a numpy array\n",
    "    x = np.array([x for (x,y) in S])\n",
    "    y = [y for (x,y) in S]\n",
    "    return x, y\n",
    "X, y = separate_labels(n_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, array([0, 1])), (False, array([0, 1])), (False, array([0, 1]))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforms boolean labels to arrays coresponsing to bolean value\n",
    "# True = [1,0]    \n",
    "# False = [0,1]\n",
    "def categorize_labels(labels):\n",
    "    array_list = []\n",
    "    for y in labels:\n",
    "        if y:\n",
    "            array_list.append([1,0])\n",
    "        else:\n",
    "            array_list.append([0,1])\n",
    "    return np.array(array_list)\n",
    "Y = categorize_labels(y)\n",
    "\n",
    "[(yi, Yi) for yi, Yi in zip(y,Y)][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Combines function so that data can be usen to train FFNN\n",
    "def generate_data_for_FFNN(params):\n",
    "    S = prepere_noisy_examples(params)\n",
    "    X, y = seperate_labels(S)\n",
    "    Y = categorize_labels(y)\n",
    "    X = np.array(X)\n",
    "    return train_test_split(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = params(t=t, dist=dist, n=10, gen_size=1000, noise=0)\n",
    "X_train, X_test, Y_train, Y_test = generate_data_for_FFNN(params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_params:\n",
    "    def __init__(self, num_layers=2, layer_size=10, activation='relu', n=10):\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "        self.n = n\n",
    "FFNN_params1 = FFNN_params(1,10)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses keras functional framework\n",
    "def create_FFNN_from_params(FFNN_params):\n",
    "    \n",
    "    inputs = keras.Input(shape=FFNN_params.n)\n",
    "    x = inputs\n",
    "    #Creates hidden layers\n",
    "    for _ in range(FFNN_params.num_layers):\n",
    "        x = layers.Dense(FFNN_params.layer_size)(x)\n",
    "    \n",
    "    # Creates output layer\n",
    "    x = layers.Dense(2, activation=FFNN_params.activation)(x)\n",
    "    \n",
    "    outputs = layers.Activation('softmax')(x)\n",
    "    \n",
    "    # Makes model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    #compiles model\n",
    "    model.compile(\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "model1 = create_FFNN_from_params(FFNN_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates different configuration for neural networks\n",
    "FFNN_params_list = []\n",
    "\n",
    "#Corespons to NN with 1 hidden layer with 10 nodes\n",
    "FFNN_params1 = FFNN_params(1,10)\n",
    "FFNN_params_list.append(FFNN_params1)\n",
    "\n",
    "FFNN_params2 = FFNN_params(5,5)\n",
    "FFNN_params_list.append(FFNN_params2)\n",
    "\n",
    "FFNN_params3 = FFNN_params(2,16,keras.activations.sigmoid)\n",
    "FFNN_params_list.append(FFNN_params3)\n",
    "\n",
    "FFNN_params4 = FFNN_params(3, 64)\n",
    "FFNN_params_list.append(FFNN_params4)\n",
    "\n",
    "FFNN_params5 = FFNN_params(20, 4)\n",
    "FFNN_params_list.append(FFNN_params5)\n",
    "\n",
    "FFNN_params6 = FFNN_params(1, 256)\n",
    "FFNN_params_list.append(FFNN_params6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [create_FFNN_from_params(FFNN_params) for FFNN_params in FFNN_params_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012206950823465982"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Returns average clasification time\n",
    "def pred_time(model, X):\n",
    "    n = len(X)\n",
    "    start = time.time()\n",
    "    model.predict(X)\n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    average_time = duration / n\n",
    "    return average_time\n",
    "pred_time(model1, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RolfS\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 1s 2ms/sample - loss: 0.7252 - acc: 0.4680\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 2s 2ms/sample - loss: 0.6375 - acc: 0.7020\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 1s 2ms/sample - loss: 0.6380 - acc: 0.7827\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 2s 2ms/sample - loss: 0.4116 - acc: 0.8240\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 3s 4ms/sample - loss: 0.3884 - acc: 0.8587\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 2s 2ms/sample - loss: 0.4054 - acc: 0.8627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.callbacks.History at 0x1980b014e48>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x1980b1df0b8>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x19810548e10>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x19810628e80>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x19810709978>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x19808066f28>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit models\n",
    "[model.fit(x=X_train, y=Y_train) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 192us/sample - loss: 0.6423 - acc: 0.5860\n",
      "250/250 [==============================] - 0s 132us/sample - loss: 0.5539 - acc: 0.8520\n",
      "250/250 [==============================] - 0s 149us/sample - loss: 0.5777 - acc: 0.9360\n",
      "250/250 [==============================] - 0s 215us/sample - loss: 0.1790 - acc: 0.9480\n",
      "250/250 [==============================] - 0s 159us/sample - loss: 0.2318 - acc: 0.9340\n",
      "250/250 [==============================] - 0s 195us/sample - loss: 0.2075 - acc: 0.9480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('num_layers: 1',\n",
       "  'size_layers: 10',\n",
       "  'time: 0.0001226669947306315',\n",
       "  'score: [0.642340765953064, 0.586]'),\n",
       " ('num_layers: 5',\n",
       "  'size_layers: 5',\n",
       "  'time: 0.00010758781433105468',\n",
       "  'score: [0.5539497175216674, 0.852]'),\n",
       " ('num_layers: 2',\n",
       "  'size_layers: 16',\n",
       "  'time: 8.900801340738932e-05',\n",
       "  'score: [0.577681824207306, 0.936]'),\n",
       " ('num_layers: 3',\n",
       "  'size_layers: 64',\n",
       "  'time: 0.00010120932261149088',\n",
       "  'score: [0.17904866647720336, 0.948]'),\n",
       " ('num_layers: 20',\n",
       "  'size_layers: 4',\n",
       "  'time: 0.00010521952311197917',\n",
       "  'score: [0.23179819393157958, 0.934]'),\n",
       " ('num_layers: 1',\n",
       "  'size_layers: 256',\n",
       "  'time: 9.498310089111328e-05',\n",
       "  'score: [0.2074899945259094, 0.948]')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate. shows number of layers, layer size, average clasificatioin time and evaluation score\n",
    "[('num_layers: '+ str(params.num_layers), \n",
    "  'size_layers: ' + str(params.layer_size), \n",
    "  'time: '+str(pred_time(model, X_train)), \n",
    "  'score: ' + str(model.evaluate(x=X_test, y=Y_test))) \n",
    " for (params, model) in zip(FFNN_params_list,models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_mistakes(model, X_test, Y_test):\n",
    "    model = models[2]\n",
    "    m_pred = np.round(model.predict(X_test))\n",
    "    mis_idxs = [i for i in range(len(m_pred)) if m_pred[i][0] != Y_test[i][0]]\n",
    "    return len(mis_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import larq as lq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular to create FFNN, but it uses QuantDense layers from larq insted of Dense layers\n",
    "def create_FFBNN_from_params(params):\n",
    "    kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "    \n",
    "    inputs = keras.Input(shape=params.n)\n",
    "    x = inputs\n",
    "    for _ in range(params.num_layers):\n",
    "        x = lq.layers.QuantDense(params.layer_size, **kwargs)(x)\n",
    "    x = lq.layers.QuantDense(2, **kwargs)(x)\n",
    "    outputs = layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "BNN_model1 = create_FFBNN_from_params(FFNN_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses same params as non binary NN so that it is easyer to compare time \n",
    "BNN_params_list = FFNN_params_list\n",
    "BNN_models = [create_FFBNN_from_params(BNN_params) for BNN_params in BNN_params_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples\n",
      "750/750 [==============================] - 2s 3ms/sample - loss: 2.2478 - acc: 0.0787\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 3s 4ms/sample - loss: 0.2838 - acc: 0.9213\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 2s 3ms/sample - loss: 0.6333 - acc: 0.8813\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 2s 3ms/sample - loss: 1.6459 - acc: 0.8147\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 6s 8ms/sample - loss: 0.7911 - acc: 0.8640\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 3s 4ms/sample - loss: 1.2098 - acc: 0.9213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.callbacks.History at 0x198163800f0>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x198163a8080>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x19819489c50>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x198194bcf60>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x198195a2b70>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x19816241c18>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit models\n",
    "[model.fit(x=X_train, y=Y_train) for model in BNN_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 3ms/sample - loss: 1.9838 - acc: 0.0520\n",
      "250/250 [==============================] - 1s 5ms/sample - loss: 0.2290 - acc: 0.9480\n",
      "250/250 [==============================] - 1s 4ms/sample - loss: 0.6945 - acc: 0.0520\n",
      "250/250 [==============================] - 1s 4ms/sample - loss: 0.2297 - acc: 0.9480\n",
      "250/250 [==============================] - 2s 8ms/sample - loss: 0.6749 - acc: 0.9480\n",
      "250/250 [==============================] - 1s 5ms/sample - loss: 0.7975 - acc: 0.9480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('num_layers: 1',\n",
       "  'size_layers: 10',\n",
       "  'time: 0.0016332708994547527',\n",
       "  'score: [1.9837779483795166, 0.052]'),\n",
       " ('num_layers: 5',\n",
       "  'size_layers: 5',\n",
       "  'time: 0.001971943219502767',\n",
       "  'score: [0.22903682374954223, 0.948]'),\n",
       " ('num_layers: 2',\n",
       "  'size_layers: 16',\n",
       "  'time: 0.0015727968215942382',\n",
       "  'score: [0.6944991278648377, 0.052]'),\n",
       " ('num_layers: 3',\n",
       "  'size_layers: 64',\n",
       "  'time: 0.0015946696599324543',\n",
       "  'score: [0.22972750997543334, 0.948]'),\n",
       " ('num_layers: 20',\n",
       "  'size_layers: 4',\n",
       "  'time: 0.003129185676574707',\n",
       "  'score: [0.6748758311271668, 0.948]'),\n",
       " ('num_layers: 1',\n",
       "  'size_layers: 256',\n",
       "  'time: 0.0014250319798787435',\n",
       "  'score: [0.7975288715362548, 0.948]')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate. shows number of layers, layer size, average clasificatioin time and evaluation score\n",
    "[('num_layers: '+ str(params.num_layers), \n",
    "  'size_layers: ' + str(params.layer_size), \n",
    "  'time: '+str(pred_time(model, X_train)), \n",
    "  'score: ' + str(model.evaluate(x=X_test, y=Y_test))) \n",
    " for (params, model) in zip(BNN_params_list,BNN_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00019371128082275392,\n",
       " 0.00012126859029134115,\n",
       " 0.0001041714350382487,\n",
       " 0.0001249996821085612,\n",
       " 0.00014876524607340496,\n",
       " 0.00010856755574544271]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pred_time(model, X_train) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001335026423136393,\n",
       " 0.00010396067301432292,\n",
       " 0.00010416666666666667,\n",
       " 0.00010416285196940104,\n",
       " 0.000130647341410319,\n",
       " 0.0001250012715657552]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pred_time(model, X_train) for model in BNN_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clasification time has no significant differance between the binary and non-binary neural netwroks. This is supprising, and I am not shure why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same parameters as earlyer but with 0.3 noise\n",
    "noisy_params = params(t=t, dist=dist, n=10, gen_size=1000, noise=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX_train, nX_test, nY_train, nY_test = generate_data_for_FFNN(noisy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 258us/sample - loss: 0.6443 - acc: 0.6607\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 252us/sample - loss: 0.4612 - acc: 0.8767\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 233us/sample - loss: 0.5408 - acc: 0.9080\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 360us/sample - loss: 0.1853 - acc: 0.9233\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 251us/sample - loss: 0.2853 - acc: 0.9080\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 651us/sample - loss: 0.2471 - acc: 0.9147\n",
      "250/250 [==============================] - 0s 152us/sample - loss: 0.6156 - acc: 0.7380\n",
      "250/250 [==============================] - 0s 180us/sample - loss: 0.3769 - acc: 0.8760\n",
      "250/250 [==============================] - 0s 172us/sample - loss: 0.5091 - acc: 0.8920\n",
      "250/250 [==============================] - 0s 403us/sample - loss: 0.1524 - acc: 0.9160\n",
      "250/250 [==============================] - 0s 192us/sample - loss: 0.3142 - acc: 0.8920\n",
      "250/250 [==============================] - 0s 284us/sample - loss: 0.2652 - acc: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 10, [0.6156107697486878, 0.738]),\n",
       " (5, 5, [0.37689025807380677, 0.876]),\n",
       " (2, 16, [0.509109901189804, 0.892]),\n",
       " (3, 64, [0.1524250770807266, 0.916]),\n",
       " (20, 4, [0.31421304678916934, 0.892]),\n",
       " (1, 256, [0.26518691778182985, 0.892])]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and evaluate FFNN\n",
    "[model.fit(x=nX_train, y=nY_train) for model in models]\n",
    "\n",
    "[(params.num_layers, params.layer_size, model.evaluate(x=nX_test, y=nY_test)) \n",
    " for (params, model) in zip(FFNN_params_list,models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 267us/sample - loss: 1.8644 - acc: 0.0853\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 213us/sample - loss: 0.2956 - acc: 0.9147\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 209us/sample - loss: 0.6627 - acc: 0.8027\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 340us/sample - loss: 0.4349 - acc: 0.8747\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 295us/sample - loss: 0.6499 - acc: 0.9147\n",
      "Train on 750 samples\n",
      "750/750 [==============================] - 0s 644us/sample - loss: 1.3123 - acc: 0.9147\n",
      "250/250 [==============================] - 0s 140us/sample - loss: 1.8029 - acc: 0.1080\n",
      "250/250 [==============================] - 0s 168us/sample - loss: 0.3424 - acc: 0.8920\n",
      "250/250 [==============================] - 0s 200us/sample - loss: 0.8636 - acc: 0.8920\n",
      "250/250 [==============================] - 0s 284us/sample - loss: 0.3427 - acc: 0.8920\n",
      "250/250 [==============================] - 0s 224us/sample - loss: 0.6434 - acc: 0.8920\n",
      "250/250 [==============================] - 0s 316us/sample - loss: 1.6564 - acc: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 10, [1.8029230136871337, 0.108]),\n",
       " (5, 5, [0.3423691029548645, 0.892]),\n",
       " (2, 16, [0.8635780806541443, 0.892]),\n",
       " (3, 64, [0.3426851944923401, 0.892]),\n",
       " (20, 4, [0.6433775095939637, 0.892]),\n",
       " (1, 256, [1.6564060850143432, 0.892])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and evaluate FFBNN\n",
    "[model.fit(x=nX_train, y=nY_train) for model in BNN_models]\n",
    "\n",
    "[(params.num_layers, params.layer_size, model.evaluate(x=nX_test, y=nY_test)) \n",
    " for (params, model) in zip(BNN_params_list,BNN_models)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00013069661458333333,\n",
       " 0.00012485122680664062,\n",
       " 0.00012095419565836589,\n",
       " 0.00012511889139811197,\n",
       " 0.00012490367889404297,\n",
       " 0.00012497774759928385]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pred_time(model, X_train) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00015012105305989583,\n",
       " 0.00024068991343180338,\n",
       " 0.00015783532460530598,\n",
       " 0.00011188189188639323,\n",
       " 0.00014583079020182293,\n",
       " 0.00017319424947102866]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pred_time(model, X_train) for model in BNN_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is worse when adding noise. It is still good given the small amount of training data. \n",
    "The FFBNN seams to do worse from short NN, getting high losses when there are few hidden layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
